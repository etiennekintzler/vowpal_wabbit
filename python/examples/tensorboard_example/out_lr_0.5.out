Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = boston.txt
num sources = 1
Enabled reductions: gd, scorer
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0   1.0000   0.0000       15
0.500000 0.000000            2            2.0   1.0000   1.0000       15
0.504930 0.509860            4            4.0   1.0000   0.5378       15
0.323170 0.141410            8            8.0   1.0000   1.0000       15
0.161594 0.000018           16           16.0   1.0000   1.0000       15
0.217465 0.273335           32           32.0   1.0000   0.4531       16
0.150157 0.082850           64           64.0   0.0000   0.0260       16
0.099326 0.048496          128          128.0   1.0000   0.8743       15
0.113688 0.128049          256          256.0   0.0000   0.1814       15
0.098885 0.084081          512          512.0   0.0000   0.0000       14
0.083218 0.067552         1024         1024.0   0.0000   0.0292       16
0.076015 0.068811         2048         2048.0   0.0000   0.0000       16
0.075771 0.075528         4096         4096.0   0.0000   0.0000       14
0.078959 0.082146         8192         8192.0   1.0000   1.0000       15
0.084353 0.089748        16384        16384.0   0.0000   0.3927       15

finished run
number of examples = 16599
weighted example sum = 16599.000000
weighted label sum = 5130.000000
average loss = 0.085618
best constant = 0.309055
best constant's loss = 0.213540
total feature number = 250958
