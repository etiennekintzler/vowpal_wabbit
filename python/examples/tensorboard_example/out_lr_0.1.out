Num weight bits = 18
learning rate = 0.1
initial_t = 0
power_t = 0.5
using no cache
Reading datafile = boston.txt
num sources = 1
Enabled reductions: gd, scorer
average  since         example        example  current  current  current
loss     last          counter         weight    label  predict features
1.000000 1.000000            1            1.0   1.0000   0.0000       15
0.684937 0.369875            2            2.0   1.0000   0.3918       15
0.658413 0.631888            4            4.0   1.0000   0.3139       15
0.496795 0.335178            8            8.0   1.0000   0.6693       15
0.298669 0.100543           16           16.0   1.0000   0.8939       15
0.269338 0.240007           32           32.0   1.0000   0.5287       16
0.211392 0.153447           64           64.0   0.0000   0.1269       16
0.155657 0.099921          128          128.0   1.0000   0.4088       15
0.168658 0.181659          256          256.0   0.0000   0.2247       15
0.142542 0.116427          512          512.0   0.0000   0.0506       14
0.123726 0.104911         1024         1024.0   0.0000   0.2134       16
0.120590 0.117453         2048         2048.0   0.0000   0.1880       16
0.124415 0.128240         4096         4096.0   0.0000   0.0000       14
0.120469 0.116523         8192         8192.0   1.0000   0.8061       15
0.123240 0.126011        16384        16384.0   0.0000   0.4377       15

finished run
number of examples = 16599
weighted example sum = 16599.000000
weighted label sum = 5130.000000
average loss = 0.125694
best constant = 0.309055
best constant's loss = 0.213540
total feature number = 250958
